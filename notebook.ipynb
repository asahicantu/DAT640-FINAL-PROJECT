{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost\n",
    "from sklearn import svm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# USER = 'elastic'\n",
    "# PASS = 'IfKREtTr7fCqMYTD8NKE4yBi'\n",
    "# REMOTE_SERVER = f'https://{USER}:{PASS}@6a0fe46eef334fada72abc91933b54e8.us-central1.gcp.cloud.es.io:9243'\n",
    "INDEX_NAME = 'ms-marco'\n",
    "\n",
    "# es = Elasticsearch(hosts=REMOTE_SERVER)\n",
    "es = Elasticsearch()\n",
    "\n",
    "\n",
    "def analyze_query(es, query, field, index='ms-marco'):\n",
    "    \"\"\"Analyzes a query with respect to the relevant index.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query: String of query terms.\n",
    "        field: The field with respect to which the query is analyzed.\n",
    "        index: Name of the index with respect to which the query is analyzed.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    tokens = es.indices.analyze(index=index, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        ## Use a boolean query to find at least one document that contains the term.\n",
    "        hits = es.search(index=index, body={'query': {'match': {field: t['token']}}},\n",
    "                         _source=False, size=1).get('hits', {}).get('hits', {})\n",
    "        doc_id = hits[0]['_id'] if len(hits) > 0 else None\n",
    "        if doc_id is None:\n",
    "            continue\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "\n",
    "def load_queries(filepath):\n",
    "    \"\"\"Loads queries from a file.\n",
    "\n",
    "        Arguments:\n",
    "            filepath: String (constructed using os.path) of the filepath to a file with queries.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with query IDs and corresponding query strings.\n",
    "    \"\"\"\n",
    "    queries = {}\n",
    "    with open(filepath, 'r', encoding=\"utf8\") as file:\n",
    "        for line in file:\n",
    "            query_id, query_text = line.strip().split('\\t')\n",
    "            if query_text is not None:\n",
    "                queries[int(query_id)] = query_text.strip()\n",
    "    return queries\n",
    "\n",
    "\n",
    "def get_doc_term_freqs(es, doc_id, field, index='toy_index'):\n",
    "    \"\"\"Gets the term frequencies of a field of an indexed document.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        doc_id: Document identifier with which the document is indexed.\n",
    "        field: Field of document to consider for term frequencies.\n",
    "        index: Name of the index where document is indexed.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of terms and their respective term frequencies in the field and document.\n",
    "    \"\"\"\n",
    "    tv = es.termvectors(index=index, id=doc_id, fields=field, term_statistics=True)\n",
    "    if tv['_id'] != doc_id:\n",
    "        return None\n",
    "    if field not in tv['term_vectors']:\n",
    "        return None\n",
    "    term_freqs = {}\n",
    "    for term, term_stat in tv['term_vectors'][field]['terms'].items():\n",
    "        term_freqs[term] = term_stat['term_freq']\n",
    "    return term_freqs\n",
    "\n",
    "\n",
    "def get_query_term_freqs(es, query_terms):\n",
    "    \"\"\"Gets the term frequencies of a list of query terms.\n",
    "\n",
    "    Arguments:\n",
    "        es: Elasticsearch object instance.\n",
    "        query_terms: List of query terms, analyzed using `analyze_query` with respect to some relevant index.\n",
    "\n",
    "    Returns:\n",
    "        A list of query terms that exist in the specified field among the documents in the index.\n",
    "    \"\"\"\n",
    "    c = Counter()\n",
    "    for term in query_terms:\n",
    "        c[term] += 1\n",
    "    return dict(c)\n",
    "\n",
    "\n",
    "def extract_query_features(query_terms, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "        Returns:\n",
    "            Dictionary with keys 'query_length', 'query_sum_idf', 'query_max_idf', and 'query_avg_idf'.\n",
    "    \"\"\"\n",
    "    q_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_features['query_length'] = 0\n",
    "        q_features['query_sum_idf'] = 0\n",
    "        q_features['query_max_idf'] = 0\n",
    "        q_features['query_avg_idf'] = 0\n",
    "        return q_features\n",
    "\n",
    "    q_features['query_length'] = len(query_terms)\n",
    "\n",
    "    count_docs_with_term = []\n",
    "    total_docs_in_index = int(es.cat.count(index=index, params={\"format\": \"json\"})[0]['count'])\n",
    "\n",
    "    for query in query_terms:\n",
    "        res = es.count(index=index, body={\n",
    "            'query':\n",
    "                {'match':\n",
    "                     {'body': query}\n",
    "                 }\n",
    "        })['count']\n",
    "        count_docs_with_term.append(res)\n",
    "\n",
    "    q_features['query_sum_idf'] = sum([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_max_idf'] = max([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "    q_features['query_avg_idf'] = np.mean([np.log(total_docs_in_index / freq) for freq in count_docs_with_term])\n",
    "\n",
    "    return q_features\n",
    "\n",
    "\n",
    "def extract_doc_features(doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a document.\n",
    "\n",
    "        Arguments:\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'doc_length_title', 'doc_length_body'.\n",
    "    \"\"\"\n",
    "    doc_features = {}\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_body'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_body'] = sum(terms.values())\n",
    "\n",
    "    terms = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    if terms is None:\n",
    "        doc_features['doc_length_title'] = 0\n",
    "    else:\n",
    "        doc_features['doc_length_title'] = sum(terms.values())\n",
    "\n",
    "    return doc_features\n",
    "\n",
    "\n",
    "def extract_query_doc_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary with keys 'unique_query_terms_in_title', 'unique_query_terms_in_body',\n",
    "            'sum_TF_title', 'sum_TF_body', 'max_TF_title', 'max_TF_body', 'avg_TF_title', 'avg_TF_body'.\n",
    "    \"\"\"\n",
    "    q_doc_features = {}\n",
    "\n",
    "    if len(query_terms) == 0:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "        return q_doc_features\n",
    "\n",
    "    terms_title = get_doc_term_freqs(es, doc_id, 'title', index)\n",
    "    terms_body = get_doc_term_freqs(es, doc_id, 'body', index)\n",
    "\n",
    "    def agg(terms_dict, query_terms_list, func):\n",
    "        freq_list = []\n",
    "        for term in query_terms_list:\n",
    "            if term in terms_dict.keys():\n",
    "                freq_list.append(terms_dict[term])\n",
    "            else:\n",
    "                freq_list.append(0)\n",
    "        return func(freq_list)\n",
    "\n",
    "    if terms_title is None:\n",
    "        q_doc_features['sum_TF_title'] = 0\n",
    "        q_doc_features['max_TF_title'] = 0\n",
    "        q_doc_features['avg_TF_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_title'] = agg(terms_title, query_terms, sum)\n",
    "        q_doc_features['max_TF_title'] = agg(terms_title, query_terms, max)\n",
    "        q_doc_features['avg_TF_title'] = agg(terms_title, query_terms, np.mean)\n",
    "\n",
    "    if terms_body is None:\n",
    "        q_doc_features['sum_TF_body'] = 0\n",
    "        q_doc_features['max_TF_body'] = 0\n",
    "        q_doc_features['avg_TF_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['sum_TF_body'] = agg(terms_body, query_terms, sum)\n",
    "        q_doc_features['max_TF_body'] = agg(terms_body, query_terms, max)\n",
    "        q_doc_features['avg_TF_body'] = agg(terms_body, query_terms, np.mean)\n",
    "\n",
    "    # UNIQUE QUERY TERMS\n",
    "    query_terms = set(query_terms)\n",
    "    if terms_title is None:\n",
    "        q_doc_features['unique_query_terms_in_title'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_title'] = len([t for t in query_terms if t in terms_title.keys()])\n",
    "    if terms_body is None:\n",
    "        q_doc_features['unique_query_terms_in_body'] = 0\n",
    "    else:\n",
    "        q_doc_features['unique_query_terms_in_body'] = len([t for t in query_terms if t in terms_body.keys()])\n",
    "\n",
    "    return q_doc_features\n",
    "\n",
    "\n",
    "FEATURES_QUERY = ['query_length', 'query_sum_idf', 'query_max_idf', 'query_avg_idf']\n",
    "FEATURES_DOC = ['doc_length_title', 'doc_length_body']\n",
    "FEATURES_QUERY_DOC = ['unique_query_terms_in_title', 'sum_TF_title', 'max_TF_title', 'avg_TF_title',\n",
    "                      'unique_query_terms_in_body', 'sum_TF_body', 'max_TF_body', 'avg_TF_body'\n",
    "                      ]\n",
    "\n",
    "\n",
    "def extract_features(query_terms, doc_id, es, index='toy_index'):\n",
    "    \"\"\"Extracts query features, document features and query-document features of a query and document pair.\n",
    "\n",
    "        Arguments:\n",
    "            query_terms: List of analyzed query terms.\n",
    "            doc_id: Document identifier of indexed document.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            List of extracted feature values in a fixed order.\n",
    "    \"\"\"\n",
    "    feature_vect = []\n",
    "\n",
    "    query_features = extract_query_features(query_terms, es, index=index)\n",
    "    for f in FEATURES_QUERY:\n",
    "        feature_vect.append(query_features[f])\n",
    "\n",
    "    doc_features = extract_doc_features(doc_id, es, index=index)\n",
    "    for f in FEATURES_DOC:\n",
    "        feature_vect.append(doc_features[f])\n",
    "\n",
    "    query_doc_features = extract_query_doc_features(query_terms, doc_id, es, index=index)\n",
    "    for f in FEATURES_QUERY_DOC:\n",
    "        feature_vect.append(query_doc_features[f])\n",
    "\n",
    "    return feature_vect"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "queries_doctrain = load_queries('data/queries.doctrain.tsv')\n",
    "queries_doctdev = load_queries('data/queries.docdev.tsv')\n",
    "QUERIES = {**queries_doctrain, **queries_doctdev}\n",
    "\n",
    "\n",
    "with open('pickles/QUERIES.pkl', 'wb') as file:\n",
    "        pickle.dump(QUERIES, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "QRELS = {}\n",
    "\n",
    "with open('data/msmarco-docdev-qrels.tsv', 'r') as file:\n",
    "    for line in file:\n",
    "        query_id, _, doc_id, _ = line.split(' ')\n",
    "        QRELS[int(query_id)] = doc_id\n",
    "\n",
    "\n",
    "with open('pickles/QRELS.pkl', 'wb') as file:\n",
    "        pickle.dump(QRELS, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def prepare_ltr_training_data(query_ids, es, index='ms-marco'):\n",
    "    \"\"\"Prepares feature vectors and labels for query and document pairs found in the training data.\n",
    "\n",
    "        Arguments:\n",
    "            query_ids: List of query IDs.\n",
    "            es: Elasticsearch object instance.\n",
    "            index: Name of relevant index on the running Elasticsearch service.\n",
    "\n",
    "        Returns:\n",
    "            X: List of feature vectors extracted for each pair of query and retrieved or relevant document.\n",
    "            y: List of corresponding labels.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for query_id in tqdm(query_ids):\n",
    "        relevent_doc = QRELS[query_id]\n",
    "        query = QUERIES[query_id]\n",
    "        analyzed_terms = analyze_query(es, query, 'body', index=index)\n",
    "\n",
    "        extracted_feature = extract_features(analyzed_terms, relevent_doc, es, index=index)\n",
    "        X.append(extracted_feature)\n",
    "        y.append(1)\n",
    "\n",
    "        hits = es.search(index=index, q=' '.join(analyzed_terms), _source=False, size=100)['hits']['hits']\n",
    "\n",
    "        for hit in hits:\n",
    "            doc_id = hit['_id']\n",
    "            if doc_id != relevent_doc:\n",
    "                extracted_feature = extract_features(analyzed_terms, doc_id, es, index=index)\n",
    "                X.append(extracted_feature)\n",
    "                y.append(0)\n",
    "    return X, y\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def load_basic_rankings_with_features(filepath, avoid_queries, es, max_size=100, index='ms-marco'):\n",
    "    basic_rankings = defaultdict(list)\n",
    "\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            record = line.split(' ')\n",
    "            query_id = int(record[0])\n",
    "            doc_id = record[2]\n",
    "\n",
    "            if query_id in avoid_queries:\n",
    "                continue\n",
    "\n",
    "            if query_id not in QRELS.keys():\n",
    "                continue\n",
    "\n",
    "            query = QUERIES[query_id]\n",
    "            analyzed_terms = analyze_query(es, query, 'body', index=index)\n",
    "\n",
    "            if len(analyzed_terms) == 0:\n",
    "                continue\n",
    "\n",
    "            extracted_feature = extract_features(analyzed_terms, doc_id, es, index=index)\n",
    "            basic_rankings[query_id].append((doc_id, extracted_feature))\n",
    "\n",
    "            if len(basic_rankings[query_id]) == 100 and len(basic_rankings) >=max_size:\n",
    "                break\n",
    "\n",
    "        return basic_rankings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "we train our models on 10% of groundtruth"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "train_query_ids = list(QRELS.keys())[:519]\n",
    "\n",
    "if os.path.isfile('pickles/training_data.pkl'):\n",
    "    with open('pickles/training_data.pkl', 'rb') as file:\n",
    "        training_data = pickle.load(file)\n",
    "else:\n",
    "    training_data = prepare_ltr_training_data(train_query_ids, es, index=INDEX_NAME)\n",
    "    with open('pickles/training_data.pkl', 'wb') as file:\n",
    "        pickle.dump(training_data, file)\n",
    "\n",
    "X_train, y_train = training_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Retrieving pre-ranked top 100 documents for each query which are not present in our training queries. So our training and testing queries remain seperate. The  mean reciprocal rank is calculated for the preranked queries. Our goal is to improve this score with our reranking algorithms."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d4328d03b11e42d8a6634f269b5481b3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('pickles/basic_rankings.pkl'):\n",
    "    with open('pickles/basic_rankings.pkl', 'rb') as file:\n",
    "        basic_rankings = pickle.load(file)\n",
    "else:\n",
    "    basic_rankings = load_basic_rankings_with_features('data/msmarco-docdev-top100.tsv',\n",
    "                                     avoid_queries=train_query_ids, max_size=100, es=es)\n",
    "    with open('pickles/basic_rankings.pkl', 'wb') as file:\n",
    "        pickle.dump(basic_rankings, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}